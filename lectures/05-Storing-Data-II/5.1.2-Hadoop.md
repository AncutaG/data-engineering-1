---
title: "Hadoop"
author: MIDS
...

---

# 

![](images/hadoop-logo-no-back-5000.png)

<div class="notes">
It has the cutest logo on the planet
</div>


# Hadoop

##

- a distributed data store
    - scalable

<div class="notes">
1000's of servers managing petabytes of data
</div>

##

- a distributed data store
    - scalable
    - resilient

<div class="notes">
You can kill off some of thosse servers
</div>

## 

- a distributed data store
    - scalable
    - resilient
    - performant

<div class="notes">
</div>

##

- a distributed data store
    - scalable
    - resilient
    - performant
- behaves like a filesystem

<div class="notes">
- fs semantics to some degree, 
- respects notions of copying, determining disk usage. 
- not some weird database with a weird structure, 
- At its heart it looks and quacks like an FS.
</div>

##

- a distributed data store
    - scalable
    - resilient
    - performant
- behaves like a filesystem
- analysis runs _next_ to the data

<div class="notes">
- unique to H when it first came out. 
- paradigm shifting in its day. 
- Rather than throwing data around to different programs, you're actually putting the analysis out to the individual data nodes themselves, you're running the analysis next to the data. 
- We will talk a lot more about this...

So, it's a distributed data store that behaves like a file system and let's you run analysis at the data.

</div>


# Pros

- open source
- standard / ubiquitous 
- supports lots of data formats & languages
- supports a wide variety of workloads
- cost effective, works great on commodity hardware

<div class="notes">
List even goes on. 
- There's a huge ecosystem around H - big pro.
- And it's just stable, it just works. 

So with all that, what are?
</div>

# Cons

- often heavyweight footprint
- resource management can be challenging

<div class="notes">
- often heavyer weight footprint than you'd like to deploy (pay for) we'll go into lighter weight options
- streaming workloads continuously running can get totlaly blocked bya  big old batch workflow that comes in. 
- Esp if you have a productionized pipeline running on a cluster and you also have ad hoc analysis being done by by data scientists running on the same cluster. 

Nowdays, it is more common to separate those workflows out - but you have to be aware of things like that, what's running and how to make them all play nicely with each other. 
</div>

# { data-background="images/hadoop.svg" }

<div class="notes">
- high level how hadoop works: Take files and break them into blocks and distribute those blocks across the datanodes in the cluster. 

-what is unique is the size of the blocks: 64-128MB per block

- ramifications of you want to try to not do lots of little files but fewer larger files, 
- but effectively it's breaking anything that you see as a file, it breaks it into blocks and spreads it across the cluster.


</div>


# { data-background="images/hadoop-distributed-and-replicated.svg" }

<div class="notes">
- NOTE that  each of those blocks gets replicated across the cluster
- if any of the data nodes dies or loses its storage or power, 
you still have replicas of that same block elsewhere througout the cluster

- able to recover the file itsef
- it's a transparent process, you're not even aware of it as a user of Hadoop.

Big points:
supports distributed execution models
inverts the traditional data execution paradigm
replicates data so you can use cheap commodity hardware without fear. 
</div>

#

<img class="logo" src="images/berkeley-school-of-information-logo.png"/>
